{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 285 - Final Project\n",
    "## Implementation of EmerDiff for Semantic Segmentation\n",
    "### Name: Pushkal Mishra\n",
    "### PID: A69033424\n",
    "\n",
    "This project is an implementation of the [EmerDiff architecture](https://kmcode1.github.io/Projects/EmerDiff/) which uses Stable Diffusion for semantic segmentation.\n",
    "\n",
    "The model is trained on the CARLA dataset, collected from an [earlier project](https://wcsng.ucsd.edu/c-shenron-demo/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from diffusers import DDIMScheduler, AutoencoderKL, UNet2DConditionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CARLA dataset\n",
    "img_path = \"/home/pushkalm11/Courses/ece285/Project/dataset/s1_2025-03-05/s1_Town01_Rep1/Town01_Scenario1_route0_03_05_14_14_18/rgb/0110.jpg\"\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image[:, 512 - 128 : 512 + 128, :]\n",
    "\n",
    "# convert to PIL image\n",
    "image = Image.fromarray(image)\n",
    "image_2 = image.resize((512, 512), resample=Image.LANCZOS)\n",
    "\n",
    "img = pil_to_tensor(image_2).cuda().unsqueeze(0).float() / 255.0 * 2.0 - 1.0\n",
    "\n",
    "# show both images side by side\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_2)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(img.min(), img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = torch.cat([img, img], dim = 0)\n",
    "# print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations for EmerDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for mask proposal\n",
    "# Negative offset for mask proposal\n",
    "lambda_1 = -10 \n",
    "# Positive offset for mask proposal\n",
    "lambda_2 = 10\n",
    "# Number of segmentation masks\n",
    "num_mask = 25\n",
    "# Text prompt for mask proposal\n",
    "text_prompt = \"\"\n",
    "\n",
    "# Compression factor for VAE to produce latent space\n",
    "vae_compress_rate = 8\n",
    "# Stable Diffusion Model checkpoint\n",
    "stable_diffusion_version = \"CompVis/stable-diffusion-v1-4\"\n",
    "# CLIP Model checkpoint\n",
    "clip_version = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "# Number of timesteps for diffusion process\n",
    "inference_time_steps = 50\n",
    "\n",
    "# Timesteps for mask proposal\n",
    "# Perform multiple runs for convergence\n",
    "time_steps = [0] * 100\n",
    "# Timestep at which the feature map is injected\n",
    "inject_mask_time_stamp = [281]\n",
    "# This is the number of timesteps to use which is scaled down from the 1000 steps in stable diffusion to the inference_time_steps\n",
    "index_to_use = int(1 + (max(inject_mask_time_stamp) // (1000 / inference_time_steps)))\n",
    "\n",
    "# k-means parameters - Standard parameters\n",
    "n_init = 100\n",
    "init_algo = \"k-means++\"\n",
    "kmeans_algo = \"lloyd\"\n",
    "\n",
    "# Layers to record feature maps - This is the first cross-attention layer at the 16x16 resolution\n",
    "record_layers = [(\"cross\", \"up\", 0, 0)]\n",
    "# Layers to inject mask offsets\n",
    "inject_mask_perturbations = [(\"cross\", \"up\", 0, 2)]\n",
    "\n",
    "# EmerDiff Config Dictionary\n",
    "emerdiff_config = {\n",
    "    \"lambda_1\": lambda_1,\n",
    "    \"lambda_2\": lambda_2,\n",
    "    \"num_mask\": num_mask,\n",
    "    \"text_prompt\": text_prompt,\n",
    "    \"time_steps\": time_steps,\n",
    "    \"inject_mask_time_stamp\": inject_mask_time_stamp,\n",
    "    \"vae_compress_rate\": vae_compress_rate,\n",
    "    \"stable_diffusion_version\": stable_diffusion_version,\n",
    "    \"clip_version\": clip_version,\n",
    "    \"inference_time_steps\": inference_time_steps,\n",
    "    \"n_init\": n_init,\n",
    "    \"init_algo\": init_algo,\n",
    "    \"kmeans_algo\": kmeans_algo,\n",
    "    \"record_layers\": record_layers,\n",
    "    \"inject_mask_perturbations\": inject_mask_perturbations,\n",
    "    \"multiplication_factor\": 0.18215,\n",
    "    \"index_to_use\": index_to_use\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VAE\n",
    "VAE = AutoencoderKL.from_pretrained(\n",
    "    stable_diffusion_version, \n",
    "    subfolder = \"vae\", \n",
    "    cache_dir = \"./\").to(\"cuda:0\")\n",
    "\n",
    "# Load DDIM scheduler for noise addition and reverse process\n",
    "ddim = DDIMScheduler.from_pretrained(\n",
    "    stable_diffusion_version, \n",
    "    subfolder = \"scheduler\", \n",
    "    cache_dir = \"./\")\n",
    "ddim.set_timesteps(\n",
    "    inference_time_steps,\n",
    "    device = \"cuda:0\")\n",
    "\n",
    "# Importing CLIP parameters\n",
    "text_tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    clip_version, \n",
    "    cache_dir = \"./\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    clip_version, \n",
    "    cache_dir = \"./\").to(\"cuda:0\")\n",
    "\n",
    "# Importing Text-Conditioned UNet\n",
    "UNet = UNet2DConditionModel.from_pretrained(\n",
    "    stable_diffusion_version, \n",
    "    subfolder = \"unet\", \n",
    "    cache_dir = \"./\").to(\"cuda:0\")\n",
    "\n",
    "# k-Means processor\n",
    "kmeans_classifier = KMeans(\n",
    "    n_clusters = emerdiff_config[\"num_mask\"], \n",
    "    init = emerdiff_config[\"init_algo\"], \n",
    "    n_init = emerdiff_config[\"n_init\"], \n",
    "    random_state = 1234, \n",
    "    algorithm = emerdiff_config[\"kmeans_algo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to swap out attention layers\n",
    "These functions are required to swap out the attention layers in the UNet architecture so that they can store and modify the attention maps.\n",
    "\n",
    "### Description of the functions:\n",
    "\n",
    "#### Function: `store_attention_maps()`\n",
    "Stores attention queries (and optionally keys) during forward passes from selected cross-attention layers. These are used for clustering mask features or preserving attention structure. Returns possibly modified query, key, and value based on the flags set.\n",
    "\n",
    "#### Function: `modify_feature_maps()`\n",
    "Applies spatial perturbations to selected feature maps using a learned binary mask and lambda weights. This helps measure how specific attention regions influence the final image. Returns the perturbed hidden states in the original shape.\n",
    "\n",
    "#### Function: `extract_mask_features()`\n",
    "Aggregates and averages stored query features across timesteps for selected layers. These features are then reshaped into vectors used for k-means clustering to generate low-resolution masks. Returns a 2D tensor of shape `[tokens, features]`.\n",
    "\n",
    "#### Function: `inject_attention()`\n",
    "Replaces default attention forward passes in the U-Net with custom logic that records or modifies attention behavior. It injects hooks into specific attention layers (cross/self, up/down/mid) to enable storing Q/K and applying feature perturbations. Does not return anything; modifies the U-Net in-place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use of each flag:\n",
    "    - record_mask_embeddings: Record the mask embeddings at the correct timestep for modification\n",
    "    - record_kqv_attention: Record the original Query and Key for the attention map so that it can be used later while reconstruction\n",
    "    - use_recorded_kqv_attention: Use the original Query and Key for the attention map so that it can be used later while reconstruction\n",
    "    - perturb_feature: Perturb the feature maps\n",
    "\"\"\"\n",
    "layer_dict = {\n",
    "    # All the flags\n",
    "    \"record_mask_embeddings\": False,\n",
    "    \"record_kqv_attention\": False,\n",
    "    \"use_recorded_kqv_attention\": False,\n",
    "    \"perturb_feature\": False,\n",
    "    # Storing the attention maps\n",
    "    \"stored_attention\": defaultdict(lambda: 0),\n",
    "    \"stored_attention_count\": defaultdict(lambda: 0),\n",
    "    \"original_kqv_attention\": {},\n",
    "    # Timestep\n",
    "    \"timestep\": -1,\n",
    "    # Layers to record\n",
    "    \"record_layers\": emerdiff_config[\"record_layers\"],\n",
    "    # Layers to perturb\n",
    "    \"perturb_layers\": emerdiff_config[\"inject_mask_perturbations\"],\n",
    "    # Mask\n",
    "    \"mask\": None,\n",
    "    # Lambda\n",
    "    \"lambda_1\": emerdiff_config[\"lambda_1\"],\n",
    "    \"lambda_2\": emerdiff_config[\"lambda_2\"]\n",
    "}\n",
    "\n",
    "# Defining a class for new attention blocks to store the maps\n",
    "class NewAttentionBlock:\n",
    "    def __init__(self, layer_dict):\n",
    "        self.layer_dict = layer_dict\n",
    "    \n",
    "    # Function to store attention maps\n",
    "    def store_attention_maps(\n",
    "        self,\n",
    "        ty = \"cross\",\n",
    "        pos = \"up\",\n",
    "        res = 0,\n",
    "        idx = 0,\n",
    "        query = None,\n",
    "        key = None,\n",
    "        value = None\n",
    "    ):\n",
    "        if self.layer_dict[\"record_mask_embeddings\"]:\n",
    "            if (ty, pos, res, idx) in self.layer_dict[\"record_layers\"]:\n",
    "                fet = query[0].clone().detach().cpu() #[h*w, 768]\n",
    "                self.layer_dict[\"stored_attention\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)] += fet\n",
    "                self.layer_dict[\"stored_attention_count\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)] += 1\n",
    "            if self.layer_dict[\"record_kqv_attention\"]:\n",
    "                # Storing the original Query and Key for the attention map\n",
    "                self.layer_dict[\"original_kqv_attention\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)] = (query.detach().clone().cpu(), key.detach().clone().cpu())\n",
    "            if self.layer_dict[\"use_recorded_kqv_attention\"]:\n",
    "                stored_query, stored_key = self.layer_dict[\"original_kqv_attention\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)]\n",
    "                query[:] = stored_query[0].to(\"cuda:0\")\n",
    "                key[:] = stored_key[0].to(\"cuda:0\")\n",
    "        return query, key, value\n",
    "\n",
    "    # Function to modify the feature maps\n",
    "    def modify_feature_maps(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        ty = \"cross\",\n",
    "        pos = \"up\",\n",
    "        res = 0,\n",
    "        idx = 0,\n",
    "        to_v = None\n",
    "    ):\n",
    "        if self.layer_dict[\"perturb_feature\"] and ((ty, pos, res, idx) in self.layer_dict[\"perturb_layers\"]):\n",
    "            original_shape = hidden_states.shape\n",
    "            if len(original_shape) == 4:\n",
    "                hidden_states = hidden_states.reshape((hidden_states.shape[0], hidden_states.shape[1], -1)).permute((0, 2, 1))\n",
    "            # Reshape the mask to the same shape as the hidden states\n",
    "            mask = self.layer_dict[\"mask\"].reshape((1, -1, 1))\n",
    "            if hidden_states.shape[0] == 2:\n",
    "                lam = torch.from_numpy(np.array([self.layer_dict[\"lambda_1\"], self.layer_dict[\"lambda_2\"]])).float().reshape(2, 1, 1).to(\"cuda:0\")\n",
    "            else:\n",
    "                lam = torch.from_numpy(np.array([self.layer_dict[\"lambda_1\"]])).float().reshape(1, 1, 1).to(\"cuda:0\")\n",
    "            \n",
    "            # Perturb the feature maps - Main contribution of the EmerDiff architecture\n",
    "            hidden_states = hidden_states + lam * mask\n",
    "            if len(original_shape) == 4:\n",
    "                hidden_states = hidden_states.permute((0,2,1)).reshape(original_shape)\n",
    "        return hidden_states\n",
    "    # Function to extract out the mask features\n",
    "    def extract_mask_features(\n",
    "        self\n",
    "    ):\n",
    "        fet = []\n",
    "        for (k, v) in self.layer_dict[\"stored_attention\"].items():\n",
    "            # Here we are taking the mean of the attention maps at the correct timestep\n",
    "            # Normalization is necessary to stabilize and avoid drift\n",
    "            fet.append(v.unsqueeze(0) / self.layer_dict[\"stored_attention_count\"][k])\n",
    "        fet = torch.cat(fet, dim=0)\n",
    "        fet = fet.permute((1,0,2))\n",
    "        fet = fet.reshape((fet.shape[0], -1))\n",
    "        return fet\n",
    "\n",
    "new_attention_block = NewAttentionBlock(layer_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: inject_attention()\n",
    "This function is used to inject the attention maps into the UNet architecture.\n",
    "\n",
    "Reference: [https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been taken from an existing github repository which follows the UNet Text-conditioned architecture\n",
    "# The code has been modified to store the attention maps and modify the feature maps\n",
    "def inject_attention(unet, new_attention_block):\n",
    "    def new_forward_attention(ty = \"cross\", pos = \"up\", res=0, idx = 0):\n",
    "        def forward(attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "            batch_size, sequence_length, _ = (\n",
    "                hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "            )\n",
    "            inner_dim = hidden_states.shape[-1]\n",
    "            query = attn.to_q(hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "            if encoder_hidden_states is None:\n",
    "                encoder_hidden_states = hidden_states\n",
    "            key = attn.to_k(encoder_hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "            value = attn.to_v(encoder_hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "            head_dim = inner_dim // attn.heads\n",
    "\n",
    "            #store qkv\n",
    "            query, key, value = new_attention_block.store_attention_maps(ty, pos, res, idx, query, key, value)\n",
    "\n",
    "            query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) #(batch, num_heads, seq_len, head_dim)\n",
    "            key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "            value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "            hidden_states = F.scaled_dot_product_attention(\n",
    "                query, key, value, attn_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim) #(batch, seq_len, num_heads*head_dim)\n",
    "            hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "            # linear proj\n",
    "            hidden_states = attn.to_out[0](hidden_states)\n",
    "\n",
    "            #perturb the output\n",
    "            hidden_states = new_attention_block.modify_feature_maps(hidden_states, ty, pos, res, idx, None)\n",
    "\n",
    "            return hidden_states\n",
    "        return forward\n",
    "    def inject_block(blocks=unet.up_blocks, pos=\"up\"):\n",
    "        #ref: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py\n",
    "        res = -1\n",
    "        if pos == \"mid\":\n",
    "            children = [blocks]\n",
    "        else:\n",
    "            children = blocks.children()\n",
    "        for net_ in children:\n",
    "            if net_.__class__.__name__ in [\"CrossAttnUpBlock2D\",\"CrossAttnDownBlock2D\",\"UNetMidBlock2DCrossAttn\"]:\n",
    "                res += 1\n",
    "                idx = -1\n",
    "                for atn in net_.attentions:\n",
    "                        if atn.__class__.__name__ == \"Transformer2DModel\":\n",
    "                            idx += 1\n",
    "                            for block in atn.transformer_blocks:\n",
    "                                if block.__class__.__name__ == \"BasicTransformerBlock\":\n",
    "                                    #self attention\n",
    "                                    if block.attn1.processor.__class__.__name__ == \"AttnProcessor2_0\":\n",
    "                                        block.attn1.processor = new_forward_attention(ty = \"self\", pos = pos, res = res, idx = idx)\n",
    "                                    #cross attention\n",
    "                                    if block.attn2.processor.__class__.__name__ == \"AttnProcessor2_0\":\n",
    "                                        block.attn2.processor = new_forward_attention(ty=\"cross\", pos = pos, res = res, idx = idx)\n",
    "        return blocks\n",
    "    unet.up_blocks = inject_block(unet.up_blocks, pos=\"up\")\n",
    "    unet.down_blocks = inject_block(unet.down_blocks, pos=\"down\")\n",
    "    unet.mid_block = inject_block(unet.mid_block, pos=\"mid\")\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injecting the attention maps into the UNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet = inject_attention(UNet, new_attention_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Forward and Reverse DDPM process\n",
    "These functions are copied over from the original implementation of the DDPM process, can be found here: [https://github.com/inbarhub/DDPM_inversion/blob/main/ddm_inversion/inversion_utils.py](https://github.com/inbarhub/DDPM_inversion/blob/main/ddm_inversion/inversion_utils.py).\n",
    "\n",
    "### Description of the functions:\n",
    "\n",
    "#### Function: `sample_xts_from_x0()`\n",
    "Generates noisy latent samples `x_t` from clean latent `x₀` by adding scheduled Gaussian noise across timesteps. Used to simulate the forward diffusion process for inversion.\n",
    "\n",
    "#### Function: `get_variance()`\n",
    "Computes the noise variance at a specific timestep using the scheduler’s cumulative alpha values. Needed for reverse sampling and DDIM step calculations.\n",
    "\n",
    "#### Function: `inversion_forward_process()`\n",
    "Performs DDPM inversion by simulating the forward process from `x₀` to `x_T`, then reconstructing noise vectors `z` via U-Net predictions at each step. Returns noisy latents, noise vectors, and timesteps.\n",
    "\n",
    "#### Function: `reverse_step()`\n",
    "Executes a single DDIM denoising step from `x_t` to `x_{t-1}` using predicted noise, diffusion schedule, and optional noise scaling. Core operation for controlled reverse generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_xts_from_x0(unet, scheduler, x0, num_inference_steps, rng):\n",
    "    alpha_bar = scheduler.alphas_cumprod\n",
    "    sqrt_one_minus_alpha_bar = (1-alpha_bar) ** 0.5\n",
    "    variance_noise_shape = (\n",
    "            num_inference_steps,\n",
    "            x0.shape[-3],\n",
    "            x0.shape[-2],\n",
    "            x0.shape[-1])\n",
    "\n",
    "    timesteps = scheduler.timesteps.to(\"cuda:0\")\n",
    "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
    "    xts = torch.zeros(variance_noise_shape).to(x0.device)\n",
    "    for t in reversed(timesteps):\n",
    "        idx = t_to_idx[int(t)]\n",
    "        # print(xts.shape, x0.shape)\n",
    "        xts[idx] = x0 * (alpha_bar[t] ** 0.5) + torch.randn(x0.shape, generator=rng).to(\"cuda:0\") * sqrt_one_minus_alpha_bar[t]\n",
    "    xts = torch.cat([xts, x0],dim = 0)\n",
    "\n",
    "    return xts\n",
    "\n",
    "def get_variance(scheduler, timestep):\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n",
    "    return variance\n",
    "\n",
    "def inversion_forward_process(unet, scheduler, x0,\n",
    "                              uncond_embedding,\n",
    "                              etas = 1.0,\n",
    "                              num_inference_steps=50,\n",
    "                              ddpm_seed = 42\n",
    "                             ):\n",
    "\n",
    "    timesteps = scheduler.timesteps.to(\"cuda:0\")\n",
    "    variance_noise_shape = (\n",
    "        num_inference_steps,\n",
    "        x0.shape[-3],\n",
    "        x0.shape[-2],\n",
    "        x0.shape[-1])\n",
    "    rng = torch.Generator().manual_seed(ddpm_seed)\n",
    "\n",
    "    etas = [etas]*scheduler.num_inference_steps\n",
    "    #generate noisy samples xts\n",
    "    # print(x0.shape)\n",
    "    xts = sample_xts_from_x0(unet, scheduler, x0, num_inference_steps=num_inference_steps, rng = rng)\n",
    "    alpha_bar = scheduler.alphas_cumprod\n",
    "    zs = torch.zeros(size=variance_noise_shape, device=\"cuda:0\") #[50, 4, 64, 64]\n",
    "\n",
    "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
    "    xt = x0\n",
    "\n",
    "    for t in timesteps:\n",
    "        idx = t_to_idx[int(t)]\n",
    "\n",
    "        xt = xts[idx][None]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = unet.forward(xt, timestep =  t, encoder_hidden_states = uncond_embedding)\n",
    "\n",
    "        noise_pred = out.sample\n",
    "\n",
    "        xtm1 =  xts[idx+1][None]\n",
    "        pred_original_sample = (xt - (1-alpha_bar[t])  ** 0.5 * noise_pred ) / alpha_bar[t] ** 0.5\n",
    "        prev_timestep = t - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "        alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "        variance = get_variance(scheduler, t)\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev - etas[idx] * variance ) ** (0.5) * noise_pred\n",
    "        mu_xt = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "        z = (xtm1 - mu_xt ) / ( etas[idx] * variance ** 0.5 )\n",
    "        zs[idx] = z\n",
    "        xtm1 = mu_xt + ( etas[idx] * variance ** 0.5 )*z\n",
    "        xts[idx+1] = xtm1\n",
    "    return xts, zs, timesteps\n",
    "\n",
    "# Simulating one step of the reverse diffusion process\n",
    "def reverse_step(scheduler, model_output, timestep, sample, eta, variance_noise):\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "    variance = get_variance(scheduler, timestep)\n",
    "    std_dev_t = eta * variance ** (0.5)\n",
    "    model_output_direction = model_output\n",
    "    pred_sample_direction = (1 - alpha_prod_t_prev - eta * variance) ** (0.5) * model_output_direction\n",
    "    prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "    if eta > 0:\n",
    "        sigma_z =  eta * variance ** (0.5) * variance_noise\n",
    "        prev_sample = prev_sample + sigma_z\n",
    "\n",
    "    return prev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main EmerDiff Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Process of Stable Diffusion along with UNet denoising to record attention maps\n",
    "def fwd_stable_diffusion(\n",
    "    latent, \n",
    "    time_steps, \n",
    "    new_attention_block):\n",
    "    # Random number generator which is shared across all timesteps\n",
    "    random_number_generator = torch.Generator().manual_seed(42)\n",
    "    # Adding noise to the latent vector at each timestep\n",
    "    for t in time_steps:\n",
    "        noise = torch.randn(latent.shape, generator = random_number_generator).to(\"cuda:0\")\n",
    "        latent_t = ddim.add_noise(latent, noise, torch.tensor([t]).int())\n",
    "        new_attention_block.layer_dict[\"timestep\"] = t\n",
    "        UNet(latent_t, t, encoder_hidden_states = new_attention_block.layer_dict[\"prompt_embedding\"])\n",
    "\n",
    "# Running one step of the reverse diffusion process\n",
    "def reverse_diffusion_one_step(\n",
    "    t, \n",
    "    latents, \n",
    "    cond1, \n",
    "    cond2 = None, \n",
    "    z_ddpm = None):\n",
    "    if cond2 != None:\n",
    "        text_embeddings  = torch.cat([cond1, cond2])\n",
    "    else:\n",
    "        text_embeddings = cond1\n",
    "    noise_pred = UNet(\n",
    "                            latents,\n",
    "                            t,\n",
    "                            encoder_hidden_states=text_embeddings\n",
    "                        ).sample\n",
    "    z_ddpm = z_ddpm.expand(latents.shape[0],-1,-1,-1) #scheduled noise\n",
    "    return reverse_step(ddim, noise_pred, t, latents, 1.0, z_ddpm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Latent Vector embedding from the image\n",
    "latent = VAE.encode(img)['latent_dist'].mean\n",
    "latent = latent * emerdiff_config[\"multiplication_factor\"]\n",
    "\n",
    "# Shape of the original image\n",
    "h, w = latent.shape[-2], latent.shape[-1]\n",
    "h, w = h * emerdiff_config[\"vae_compress_rate\"], w * emerdiff_config[\"vae_compress_rate\"]\n",
    "\n",
    "# Adding text embedding to the layer_dict\n",
    "text_prompt_tokens = text_tokenizer(\n",
    "    text_prompt, \n",
    "    padding = \"max_length\", \n",
    "    max_length = text_tokenizer.model_max_length, \n",
    "    truncation = True, \n",
    "    return_tensors = \"pt\")\n",
    "text_prompt_embeddings = text_encoder(text_prompt_tokens.input_ids.to(\"cuda:0\"))[0]\n",
    "print(f\"Shape of prompt embeddings in CLIP space: {text_prompt_embeddings.shape}\")\n",
    "new_attention_block.layer_dict[\"prompt_embedding\"] = text_prompt_embeddings\n",
    "# new_attention_block.layer_dict[\"prompt_embedding\"] = torch.cat([text_prompt_embeddings, text_prompt_embeddings], dim = 0)\n",
    "\n",
    "# Extracting mask for clustering\n",
    "new_attention_block.layer_dict[\"record_mask_embeddings\"] = True\n",
    "fwd_stable_diffusion(latent, emerdiff_config[\"time_steps\"], new_attention_block)\n",
    "new_attention_block.layer_dict[\"record_mask_embeddings\"] = False\n",
    "\n",
    "# Clustering the features to produce low-resolution segmentation maps\n",
    "mask_features = new_attention_block.extract_mask_features()\n",
    "kmeans_classifier.fit(mask_features)\n",
    "mask_to_id_mapping = torch.from_numpy(kmeans_classifier.labels_).to(\"cuda:0\")\n",
    "num_masks = mask_to_id_mapping.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the forward process and track the latents and noise at the timesteps to be used for the final image\n",
    "index_to_use = emerdiff_config[\"index_to_use\"]\n",
    "xts, zs, timesteps = inversion_forward_process(\n",
    "    UNet,\n",
    "    ddim,\n",
    "    latent,\n",
    "    new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "    etas = 1.0,\n",
    "    num_inference_steps = emerdiff_config[\"inference_time_steps\"],\n",
    "    ddpm_seed = 42\n",
    ")\n",
    "xts = xts.unsqueeze(1)\n",
    "# Excluding the first timestep\n",
    "xts = xts[-index_to_use-1:-1]\n",
    "timesteps = timesteps[-index_to_use:]\n",
    "zs = zs[-index_to_use:]\n",
    "\n",
    "# Run the reverse process to get the de-noised latents at the timesteps to be used for the final image\n",
    "with torch.no_grad():\n",
    "    new_attention_block.layer_dict[\"record_kqv_attention\"] = True\n",
    "    original_latents = xts[0]\n",
    "    for i, (xt, t) in enumerate(zip(xts, timesteps)):\n",
    "        new_attention_block.layer_dict[\"timestep\"] = t\n",
    "        original_latents = reverse_diffusion_one_step(\n",
    "            t = t,\n",
    "            latents = original_latents,\n",
    "            cond1 = new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "            z_ddpm = zs[i]\n",
    "        )\n",
    "    new_attention_block.layer_dict[\"record_kqv_attention\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the de-noised latents, we can modulate the attention maps for every mask to generate the final image\n",
    "img_with_mask_id = []\n",
    "for i in range(emerdiff_config[\"num_mask\"]):\n",
    "    print(f\"Generating the difference map for mask id: {i + 1} / {emerdiff_config['num_mask']}\")\n",
    "    with torch.no_grad():\n",
    "        # Here we generate the difference map for the mask id i and store them in a list\n",
    "        # Updating the mask\n",
    "        new_attention_block.layer_dict[\"mask\"] = (mask_to_id_mapping == i).float().to(\"cuda:0\")\n",
    "        \n",
    "        # Modifying the latent and generating the difference map\n",
    "        latent_to_modify = torch.cat([xts[0]]*2, dim = 0)\n",
    "        # Setting the flag to use the recorded attention maps\n",
    "        new_attention_block.layer_dict[\"use_recorded_kqv_attention\"] = True\n",
    "        for i, (xt, t) in enumerate(zip(xts, timesteps)):\n",
    "            # Setting the timestep\n",
    "            new_attention_block.layer_dict[\"timestep\"] = t\n",
    "            # Setting the flag to perturb the feature maps\n",
    "            new_attention_block.layer_dict[\"perturb_feature\"] = (t in emerdiff_config[\"inject_mask_time_stamp\"])\n",
    "            # Running the diffusion step\n",
    "            latent_to_modify = reverse_diffusion_one_step(\n",
    "                t = t,\n",
    "                latents = latent_to_modify,\n",
    "                cond1 = new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "                cond2 = new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "                z_ddpm = zs[i]\n",
    "            )\n",
    "        new_attention_block.layer_dict[\"use_recorded_kqv_attention\"] = False\n",
    "        new_attention_block.layer_dict[\"perturb_feature\"] = False\n",
    "        x_modified = VAE.decode(latent_to_modify / emerdiff_config[\"multiplication_factor\"]).sample\n",
    "        \n",
    "        # Adding blurring to the image\n",
    "        x_modified = torchvision.transforms.functional.gaussian_blur(x_modified, kernel_size = 3)\n",
    "        difference_map = torch.linalg.norm(x_modified[0 : 1] - x_modified[1 : 2], dim = 1)\n",
    "        img_with_mask_id.append(difference_map.cpu())\n",
    "    \n",
    "# Concatenating the difference maps and taking argmax to get the final image\n",
    "segmented_image = torch.argmax(torch.cat(img_with_mask_id, dim = 0), dim = 0)\n",
    "print(f\"Shape of the segmented image: {segmented_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = plt.get_cmap('tab20', torch.max(segmented_image.flatten())+1)\n",
    "# col_img = cmap(segmented_image)[:, :, :3]  # shape: (H, W, 3), values in [0,1]\n",
    "# col_img = (col_img * 255).astype(np.uint8)\n",
    "# plt.figure()\n",
    "# plt.imshow(col_img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "palette = np.random.choice(range(256), size=(1000,3)).astype(np.uint8)\n",
    "col_img = palette[segmented_image.flatten()].reshape(segmented_image.shape+(3,))\n",
    "im = np.array(col_img)*0.8+((img[0]+1.0)/2.0*255.0).permute((1,2,0)).cpu().numpy()*0.2\n",
    "plt.imshow(Image.fromarray(im.astype(np.uint8)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Process\n",
    "\n",
    "TODO:\n",
    "1. Describe the entire forward process here\n",
    "2. Talk about VAE and noise addition according to the DDPM scheduler\n",
    "3. Provide mathematical equations for this\n",
    "\n",
    "Use of VAE:\n",
    "- A meaningful, structured latent space\n",
    "- The VAE maps high-dimensional images (e.g. 512×512×3) into lower-dimensional, semantic latent variables (e.g. 64×64×4).\n",
    "- This latent space is:\n",
    "  - Continuous and smooth\n",
    "  - More semantically meaningful (similar images → nearby latents)\n",
    "  - Easier for the diffusion model to learn in\n",
    "- Without a VAE, the diffusion model would need to operate directly in image space, which is harder, slower, and requires more memory.\n",
    "- Efficiency (Compression!)\n",
    "  - Stable Diffusion doesn't run diffusion in pixel space — it uses a latent diffusion model (LDM).\n",
    "  - Instead of operating on 512×512×3 (≈786K values), it works on 64×64×4 (≈16K values)\n",
    "  - That’s ~50× smaller\n",
    "- This allows:\n",
    "  - Faster training and inference\n",
    "  - Memory-efficient denoising steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need a probabilistic model and not a deterministic one?\n",
    "In generative modeling (and especially diffusion models), we don’t just want a single latent representation — we want:\n",
    "\n",
    "- A distribution over possible representations\n",
    "\n",
    "- The ability to sample diverse outputs from the same input\n",
    "\n",
    "- A smooth latent space where nearby points yield semantically similar images\n",
    "\n",
    "Without a probabilistic latent space:\n",
    "- Interpolating between images becomes meaningless or discontinuous\n",
    "- Sampling new data becomes impossible (you can't sample from a deterministic encoder)\n",
    "- Inversion (mapping image → latent) becomes unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Process - Denoising using DDPM and EmerDiff modifications\n",
    "\n",
    "### Exaplaination:\n",
    "1. Here we add noise in the reverse process as proposed in the DDPM paper.\n",
    "2. This helps model the true posterior distribution of the diffusion process.\n",
    "3. If we were not to add the noise, the reverse process would be deterministic and so not a good approximation of the true posterior distribution.\n",
    "\n",
    "TODO:\n",
    "1. Add reference to DDPM paper\n",
    "2. Explain the logic behind this \n",
    "3. Provide mathematical equations for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the art semantic segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "# Prepare image\n",
    "inputs = processor(images=image_2, return_tensors=\"pt\")\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_classes, height, width)\n",
    "\n",
    "# Get predicted segmentation map\n",
    "upsampled_logits = torch.nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image_2.size[::-1],  # (height, width)\n",
    "    mode='bilinear',\n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_2)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pred_seg.cpu())\n",
    "plt.title(\"Segmentation Map\") \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"EmerDiff example implementation.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1Kl1tnHYo2OaUY9hdKFP4wfIpTe-WHaSp\n",
    "\n",
    "This notebook provides the **minimal** re-implementation of \"EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models\". **We are currently working on improving the notebook.** If you spot a bug, please let us know. For more details, check out our project page: https://kmcode1.github.io/Projects/EmerDiff/\n",
    "\n",
    "# Set up environment\n",
    "\"\"\"\n",
    "\n",
    "# !pip install diffusers\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %matplotlib inline\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from math import sqrt\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMScheduler, DDIMScheduler, StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#DDPM-inversion\n",
    "#Code mostly copied from: https://github.com/inbarhub/DDPM_inversion/blob/main/ddm_inversion/inversion_utils.py\n",
    "#Reference paper: https://arxiv.org/pdf/2304.06140.pdf\n",
    "\n",
    "def add_noise(scheduler, original_samples, next_timestep, rng): #add noise from prev_timestep -> next_timestep\n",
    "    noise = torch.randn(original_samples.shape, generator=rng).to(\"cuda:0\")\n",
    "\n",
    "    prev_timestep = next_timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    alpha_prod_t_next = scheduler.alphas_cumprod[next_timestep]\n",
    "\n",
    "    alpha = alpha_prod_t_next / alpha_prod_t_prev\n",
    "    alpha = alpha.to(\"cuda:0\")\n",
    "\n",
    "    #ref: https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L477\n",
    "    sqrt_alpha_prod = alpha ** 0.5\n",
    "    sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "    while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    sqrt_one_minus_alpha_prod = (1 - alpha) ** 0.5\n",
    "    sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "    while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise #x_{prev} -> x_{next}\n",
    "    return noisy_samples\n",
    "\n",
    "def sample_xts_from_x0(unet, scheduler, x0, num_inference_steps, rng):\n",
    "    alpha_bar = scheduler.alphas_cumprod\n",
    "    sqrt_one_minus_alpha_bar = (1-alpha_bar) ** 0.5\n",
    "    variance_noise_shape = (\n",
    "            num_inference_steps,\n",
    "            x0.shape[-3],\n",
    "            x0.shape[-2],\n",
    "            x0.shape[-1])\n",
    "\n",
    "    timesteps = scheduler.timesteps.to(\"cuda:0\")\n",
    "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
    "    xts = torch.zeros(variance_noise_shape).to(x0.device)\n",
    "    for t in reversed(timesteps):\n",
    "        idx = t_to_idx[int(t)]\n",
    "        xts[idx] = x0 * (alpha_bar[t] ** 0.5) + torch.randn(x0.shape, generator=rng).to(\"cuda:0\") * sqrt_one_minus_alpha_bar[t]\n",
    "    xts = torch.cat([xts, x0 ],dim = 0)\n",
    "\n",
    "    return xts\n",
    "\n",
    "def get_variance(scheduler, timestep):\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n",
    "    return variance\n",
    "\n",
    "def inversion_forward_process(unet, scheduler, x0,\n",
    "                              uncond_embedding,\n",
    "                              etas = 1.0,\n",
    "                              num_inference_steps=50,\n",
    "                              ddpm_seed = 40\n",
    "                             ):\n",
    "\n",
    "    timesteps = scheduler.timesteps.to(\"cuda:0\")\n",
    "    variance_noise_shape = (\n",
    "        num_inference_steps,\n",
    "        x0.shape[-3],\n",
    "        x0.shape[-2],\n",
    "        x0.shape[-1])\n",
    "    rng = torch.Generator().manual_seed(ddpm_seed)\n",
    "\n",
    "    etas = [etas]*scheduler.num_inference_steps\n",
    "    #generate noisy samples xts\n",
    "    xts = sample_xts_from_x0(unet, scheduler, x0, num_inference_steps=num_inference_steps, rng = rng)\n",
    "    alpha_bar = scheduler.alphas_cumprod\n",
    "    zs = torch.zeros(size=variance_noise_shape, device=\"cuda:0\") #[50, 4, 64, 64]\n",
    "\n",
    "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
    "    xt = x0\n",
    "\n",
    "    for t in timesteps:\n",
    "        idx = t_to_idx[int(t)]\n",
    "\n",
    "        xt = xts[idx][None]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = unet.forward(xt, timestep =  t, encoder_hidden_states = uncond_embedding)\n",
    "\n",
    "        noise_pred = out.sample\n",
    "\n",
    "        xtm1 =  xts[idx+1][None]\n",
    "        pred_original_sample = (xt - (1-alpha_bar[t])  ** 0.5 * noise_pred ) / alpha_bar[t] ** 0.5\n",
    "        prev_timestep = t - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "        alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "        variance = get_variance(scheduler, t)\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev - etas[idx] * variance ) ** (0.5) * noise_pred\n",
    "        mu_xt = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "        z = (xtm1 - mu_xt ) / ( etas[idx] * variance ** 0.5 )\n",
    "        zs[idx] = z\n",
    "        xtm1 = mu_xt + ( etas[idx] * variance ** 0.5 )*z\n",
    "        xts[idx+1] = xtm1\n",
    "    return xts, zs, timesteps\n",
    "\n",
    "def reverse_step(scheduler, model_output, timestep, sample, eta, variance_noise):\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "    variance = get_variance(scheduler, timestep)\n",
    "    std_dev_t = eta * variance ** (0.5)\n",
    "    model_output_direction = model_output\n",
    "    pred_sample_direction = (1 - alpha_prod_t_prev - eta * variance) ** (0.5) * model_output_direction\n",
    "    prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "    if eta > 0:\n",
    "        sigma_z =  eta * variance ** (0.5) * variance_noise\n",
    "        prev_sample = prev_sample + sigma_z\n",
    "\n",
    "    return prev_sample\n",
    "\n",
    "#utils\n",
    "def load_img(path, resize_to_512 = False):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    if resize_to_512:\n",
    "      w, h = 512, 512\n",
    "    else:\n",
    "      fac = sqrt(512*512/w/h)\n",
    "      w = int(w*fac)\n",
    "      h = int(h*fac)\n",
    "      w, h = map(lambda x: x - x % 64, (w + 63, h + 63))  # resize to integer multiple of 64\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2. * image - 1.\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "class Controller:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.record_mask_proposal = False #record feature maps for clustering\n",
    "        self.record_attention = False #record original q/k/v\n",
    "        self.use_recorded_attention = False #use recorded original attention to ensure better reconstruction\n",
    "        self.perturb_feature = False #perturb feature map\n",
    "\n",
    "        self.store = defaultdict(lambda : 0) #for mask proposal\n",
    "        self.store_cnt = defaultdict(lambda : 0)\n",
    "        self.orig = {} #for recording original q/k/v\n",
    "\n",
    "        self.t = -1\n",
    "\n",
    "        self.h = self.w = 512 #resolution of image\n",
    "        self.compress_rate = config.compress_rate #resolution of image/resolution of latent\n",
    "\n",
    "        self.lam1 = config.lam1\n",
    "        self.lam2 = config.lam2\n",
    "\n",
    "        return\n",
    "\n",
    "    def extract_mask_feature(self):\n",
    "        fet = [] #[h*w,768]\n",
    "        for (k, v) in self.store.items():\n",
    "            fet.append(v.unsqueeze(0)/self.store_cnt[k]) #take mean\n",
    "        self.store = defaultdict(lambda : 0)\n",
    "        self.store_cnt = defaultdict(lambda : 0)\n",
    "        fet = torch.cat(fet,dim=0) #(sample, h*w, 768)\n",
    "        fet = fet.permute((1,0,2)) #(h*w, sample, 768)\n",
    "        fet = fet.reshape((fet.shape[0], -1)) #(h*w, sample*768)\n",
    "\n",
    "        return fet\n",
    "\n",
    "    def store_attn(self, ty, pos, res, idx, query, key, val):\n",
    "        # print(ty, pos, res, idx)\n",
    "        # print(self.config.record_mask_proposal_layers)\n",
    "        if self.record_mask_proposal:\n",
    "            if (ty, pos, res, idx) in self.config.record_mask_proposal_layers:\n",
    "                fet = query[0].clone().detach().cpu() #[h*w, 768]\n",
    "                self.store[(self.t,ty,pos,res,idx)] += fet\n",
    "                self.store_cnt[(self.t,ty,pos,res,idx)] += 1\n",
    "        if self.record_attention:  #save cross/self attention map\n",
    "            self.orig[(self.t, ty, pos, res, idx)] = (query.detach().clone().cpu(), key.detach().clone().cpu())\n",
    "        if self.use_recorded_attention: #preserve cross/self attention map\n",
    "            query_, key_= self.orig[(self.t, ty, pos, res, idx)]\n",
    "            query[:] = query_[0].to(\"cuda:0\")\n",
    "            key[:] = key_[0].to(\"cuda:0\")\n",
    "        return query, key, val\n",
    "\n",
    "    def modify_feature(self, hidden_states, ty, pos, res, idx, to_v = None):\n",
    "        if self.perturb_feature and ((ty, pos, res, idx) in self.config.perturb_feature_layers):\n",
    "            original_shape = hidden_states.shape\n",
    "            if len(original_shape) == 4: #[2, 1280, 16, 16]\n",
    "                hidden_states = hidden_states.reshape((hidden_states.shape[0], hidden_states.shape[1], -1)).permute((0,2,1)) #[2,h*w,1280]\n",
    "            mask = self.mask.reshape((1,-1,1)) #[1,h*w,1]\n",
    "            if hidden_states.shape[0] == 2:\n",
    "                lam = torch.from_numpy(np.array([self.lam1, self.lam2])).float().reshape(2,1,1).to(\"cuda:0\")\n",
    "            else:\n",
    "                lam = torch.from_numpy(np.array([self.lam1])).float().reshape(1,1,1).to(\"cuda:0\")\n",
    "            hidden_states = hidden_states + lam*mask\n",
    "            if len(original_shape) == 4:\n",
    "                hidden_states = hidden_states.permute((0,2,1)).reshape(original_shape)\n",
    "        return hidden_states\n",
    "\n",
    "def inject_attention(unet, controller):\n",
    "        def new_forward_attention(ty = \"cross\", pos = \"up\", res=0, idx = 0):\n",
    "             def forward(attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "                batch_size, sequence_length, _ = (\n",
    "                    hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "                )\n",
    "                inner_dim = hidden_states.shape[-1]\n",
    "                query = attn.to_q(hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "                if encoder_hidden_states is None:\n",
    "                    encoder_hidden_states = hidden_states\n",
    "                key = attn.to_k(encoder_hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "                value = attn.to_v(encoder_hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "                head_dim = inner_dim // attn.heads\n",
    "\n",
    "                #store qkv\n",
    "                query, key, value = controller.store_attn(ty, pos, res, idx, query, key, value)\n",
    "\n",
    "                query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) #(batch, num_heads, seq_len, head_dim)\n",
    "                key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "                value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "                hidden_states = F.scaled_dot_product_attention(\n",
    "                    query, key, value, attn_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim) #(batch, seq_len, num_heads*head_dim)\n",
    "                hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "                # linear proj\n",
    "                hidden_states = attn.to_out[0](hidden_states)\n",
    "\n",
    "                #perturb the output\n",
    "                hidden_states = controller.modify_feature(hidden_states,ty, pos, res, idx, None)\n",
    "\n",
    "                return hidden_states\n",
    "             return forward\n",
    "        def inject_block(blocks=unet.up_blocks, pos=\"up\"):\n",
    "            #ref: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py\n",
    "            res = -1\n",
    "            if pos == \"mid\":\n",
    "                children = [blocks]\n",
    "            else:\n",
    "                children = blocks.children()\n",
    "            for net_ in children:\n",
    "                if net_.__class__.__name__ in [\"CrossAttnUpBlock2D\",\"CrossAttnDownBlock2D\",\"UNetMidBlock2DCrossAttn\"]:\n",
    "                    res += 1\n",
    "                    idx = -1\n",
    "                    for atn in net_.attentions:\n",
    "                           if atn.__class__.__name__ == \"Transformer2DModel\":\n",
    "                                idx += 1\n",
    "                                for block in atn.transformer_blocks:\n",
    "                                    if block.__class__.__name__ == \"BasicTransformerBlock\":\n",
    "                                        #self attention\n",
    "                                        if block.attn1.processor.__class__.__name__ == \"AttnProcessor2_0\":\n",
    "                                            block.attn1.processor = new_forward_attention(ty = \"self\", pos = pos, res = res, idx = idx)\n",
    "                                        #cross attention\n",
    "                                        if block.attn2.processor.__class__.__name__ == \"AttnProcessor2_0\":\n",
    "                                            block.attn2.processor = new_forward_attention(ty=\"cross\", pos = pos, res = res, idx = idx)\n",
    "        inject_block(unet.up_blocks, pos=\"up\")\n",
    "        inject_block(unet.down_blocks, pos=\"down\")\n",
    "        inject_block(unet.mid_block, pos=\"mid\")\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        #load model\n",
    "        self.ddim_scheduler = DDIMScheduler().from_pretrained(config.stable_version, subfolder=\"scheduler\", cache_dir=config.cache_dir)\n",
    "        self.ddim_scheduler.set_timesteps(config.inference_steps, device=\"cuda:0\")\n",
    "        self.vae = AutoencoderKL.from_pretrained(config.stable_version, subfolder=\"vae\",cache_dir=config.cache_dir).to(\"cuda:0\")\n",
    "        requires_grad(self.vae, False)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(config.clip_version, cache_dir=config.cache_dir)\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(config.clip_version, cache_dir=config.cache_dir).to(\"cuda:0\")\n",
    "        requires_grad(self.text_encoder, False)\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(config.stable_version, subfolder=\"unet\", cache_dir=config.cache_dir).to(\"cuda:0\")\n",
    "        requires_grad(self.unet, False)\n",
    "\n",
    "        self.uncond = self.get_text_embedding([config.uncond_words])\n",
    "\n",
    "        #load attention controller and inject\n",
    "        self.controller = Controller(config)\n",
    "        inject_attention(self.unet, self.controller)\n",
    "\n",
    "    def latent2tensor(self, latents):\n",
    "        x_samples = self.vae.decode(latents / 0.18215).sample #[1,3,512,512]\n",
    "        return x_samples\n",
    "    def tensor2latent(self, image):\n",
    "        latents = self.vae.encode(image)['latent_dist'].mean\n",
    "        latents = latents * 0.18215\n",
    "        return latents\n",
    "    def get_text_embedding(self, prompt, device=\"cuda:0\"):\n",
    "        tokens = self.tokenizer(prompt, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        for i in range(1):\n",
    "            text_embeddings = self.text_encoder(tokens.input_ids.to(device))[0] #[1, 77, 768]\n",
    "            return text_embeddings\n",
    "    def record_ddpm(self, z0, tims):\n",
    "        #Add noises to z0 under the shared seed at tims and run unet\n",
    "        rng = torch.Generator().manual_seed(self.config.ddpm_seed) #shared noise\n",
    "        for t in tims:\n",
    "            if t < 0:\n",
    "                lat, t = init_latent, 0\n",
    "            else:\n",
    "                noise = torch.randn(z0.shape, generator=rng).to(\"cuda:0\")\n",
    "                zt = self.ddim_scheduler.add_noise(z0, noise=noise, timesteps = torch.tensor([t]).int())\n",
    "            self.controller.t = t\n",
    "            self.unet(zt, t, encoder_hidden_states = self.uncond)\n",
    "    def diffusion_step(self, t, latents, cond1, cond2 = None, z_ddpm = None):\n",
    "        #diffusion_step: x_t -> x_{t-1}\n",
    "        if cond2 != None:\n",
    "          text_embeddings  = torch.cat([cond1, cond2])\n",
    "        else:\n",
    "          text_embeddings = cond1\n",
    "        noise_pred = self.unet(\n",
    "                                latents,\n",
    "                                t,\n",
    "                                encoder_hidden_states=text_embeddings\n",
    "                            ).sample\n",
    "        z_ddpm = z_ddpm.expand(latents.shape[0],-1,-1,-1) #scheduled noise\n",
    "        return reverse_step(self.ddim_scheduler, noise_pred, t, latents, 1.0, z_ddpm) #ddpm_inversion\n",
    "\n",
    "\"\"\"# Config\"\"\"\n",
    "\n",
    "class ConfigBase:\n",
    "    #Model Config\n",
    "    stable_version = \"CompVis/stable-diffusion-v1-4\"\n",
    "    clip_version = \"openai/clip-vit-large-patch14\"\n",
    "    cache_dir = \"./\"\n",
    "    compress_rate = 32 #(input image resolution) / (cross-attention resolution)\n",
    "    vae_compress_rate = 8 #(input image resolution) / (latent resolution)\n",
    "\n",
    "    #Method Config\n",
    "    inference_steps = 50 #total inference steps\n",
    "\n",
    "    #For mask proposal\n",
    "    lam1 = -10 #negative offset\n",
    "    lam2 = 10 #positive offset\n",
    "    num_mask = 15 #number of segmentation masks\n",
    "    k_means_seed = 1234\n",
    "    uncond_words =  ''\n",
    "    n_init = 100 # use \"auto\" in practice\n",
    "    init_algo = \"k-means++\"\n",
    "    kmeans_algo = \"lloyd\"\n",
    "    tims_mask = [0]*100 #timesteps for mask proposal. Better to sample several times for convergence. Reduce it in practice.\n",
    "    inject_offset_tims = [281] #timesteps to modulaate feature maps (i.e. inject offsets)\n",
    "\n",
    "    #For feature extraction\n",
    "    ddpm_seed = 42 #we use shared ddpm noise\n",
    "\n",
    "    #Create palatte (todo)\n",
    "    np.random.seed(123)\n",
    "    palette = np.random.choice(range(256), size=(1000,3)).astype(np.uint8)\n",
    "\n",
    "    #Layers\n",
    "    record_mask_proposal_layers = [(\"cross\", \"up\", 0, 0)] #(ty, pos, res, idx), layers to record feature maps\n",
    "    perturb_feature_layers = [(\"cross\", \"up\", 0, 2)] #layers to inject noise\n",
    "\n",
    "\"\"\"# Main pipeline\"\"\"\n",
    "\n",
    "class MaskProposal:\n",
    "    num_mask = None #number of masks\n",
    "    ids_img = None #high-resolution segmentation maps: e.g. [512*512] -> mask_id\n",
    "    ids_hidden = None #low-resolution segmentation maps: e.g. [16*16] -> mask_id\n",
    "\n",
    "    def save_cluster(self, clusters, filename):\n",
    "        col = self.config.palette[clusters.flatten()].reshape(clusters.shape+(3,))\n",
    "        label = Image.fromarray(col).resize((self.w, self.h), Image.NEAREST)\n",
    "        label.save(filename)\n",
    "\n",
    "    def __init__(self, config, model, init_latent):\n",
    "\n",
    "        self.config = config\n",
    "        self.h = init_latent.shape[-2]*config.vae_compress_rate\n",
    "        self.w = init_latent.shape[-1]*config.vae_compress_rate\n",
    "\n",
    "        #Step 1. extract mask features maps for clustering\n",
    "        model.controller.record_mask_proposal = True\n",
    "        model.record_ddpm(init_latent, config.tims_mask)\n",
    "        model.controller.record_mask_proposal = False\n",
    "\n",
    "        #Step 2. cluster feature maps to produce low-resolution segmentation maps\n",
    "        print(\"Perform K-means and generate low resolution masks..\")\n",
    "        mask_fet = model.controller.extract_mask_feature() #[16*16, 1280]\n",
    "        kmeans = KMeans(n_clusters=config.num_mask, init=config.init_algo, n_init=config.n_init, random_state = config.k_means_seed, algorithm=config.kmeans_algo).fit(mask_fet.numpy())\n",
    "        #self.save_cluster(kmeans.labels_.reshape((self.h//config.compress_rate, self.w//config.compress_rate)), \"./mask_proposal.png\")\n",
    "        self.ids_hidden = torch.from_numpy(kmeans.labels_)\n",
    "\n",
    "        self.num_mask = self.ids_hidden.max()+1\n",
    "\n",
    "        #Perform DDPM inversion\n",
    "        print(\"Perform DDPM inversion..\")\n",
    "        invert_until = int(max(config.inject_offset_tims)//20+1)\n",
    "        xts, zs, ts = inversion_forward_process(model.unet, model.ddim_scheduler, init_latent, model.uncond, 1.0, config.inference_steps,  ddpm_seed = config.ddpm_seed)\n",
    "        xts = xts.unsqueeze(1)\n",
    "        xts = xts[-invert_until-1:-1] #excludes x0\n",
    "        ts = ts[-invert_until:]\n",
    "        zs = zs[-invert_until:]\n",
    "        original_latent_t = list(zip(xts, ts))\n",
    "\n",
    "        #Store original attention maps\n",
    "        with torch.no_grad():\n",
    "            model.controller.record_attention = True\n",
    "            original_latents = original_latent_t[0][0]\n",
    "            for i, (_, t) in enumerate(original_latent_t):\n",
    "                model.controller.t = t\n",
    "                original_latents = model.diffusion_step(t, original_latents, model.uncond, z_ddpm = zs[i])\n",
    "            model.controller.record_attention = False\n",
    "\n",
    "        #Step 3. Now, perform modulated diffusion process for each low-resolution mask\n",
    "        print(\"Now generating pixel-level segmentation maps..\")\n",
    "        def sample_loop(latents):\n",
    "            latents = torch.cat([latents]*2,dim=0)\n",
    "            model.controller.use_recorded_attention = True\n",
    "            with torch.no_grad():\n",
    "                #Modulated denoising loop\n",
    "                for i, (_, t) in enumerate(original_latent_t):\n",
    "                    model.controller.t = t\n",
    "                    model.controller.perturb_feature = (t in config.inject_offset_tims) #Modify attention map\n",
    "                    latents = model.diffusion_step(t, latents, model.uncond, model.uncond, z_ddpm = zs[i])\n",
    "            model.controller.use_recorded_attention = False\n",
    "            model.controller.perturb_feature = False\n",
    "            x_edited = model.latent2tensor(latents) #[2,3,512,512]\n",
    "            return x_edited\n",
    "\n",
    "        def generate_dif_map(cluster_id):\n",
    "            print(\"Peform modulated denoising process for mask id:\", cluster_id)\n",
    "            model.controller.mask = (self.ids_hidden == cluster_id).float().to(\"cuda:0\") #[16,16]\n",
    "            x_edited = sample_loop(original_latent_t[0][0])\n",
    "            x_edited = torchvision.transforms.functional.gaussian_blur(x_edited, kernel_size=3)\n",
    "            dif_map = torch.linalg.norm(x_edited[0:1]-x_edited[1:2], dim=1)\n",
    "            return dif_map #[1,512,512]\n",
    "\n",
    "        lis_img = []\n",
    "        for i in range(config.num_mask):\n",
    "            with torch.no_grad():\n",
    "                lis_img.append(generate_dif_map(i).cpu())\n",
    "        self.ids_img = torch.argmax(torch.cat(lis_img,dim=0),dim=0) #[HxW]\n",
    "\n",
    "\"\"\"# Load image\"\"\"\n",
    "\n",
    "# Write an equivalent installation of the following code:\n",
    "# !wget https://raw.githubusercontent.com/NVlabs/ODISE/main/demo/examples/coco.jpg\n",
    "os.system(\"wget https://raw.githubusercontent.com/NVlabs/ODISE/main/demo/examples/coco.jpg\")\n",
    "\n",
    "# img = load_img(path=\"./coco.jpg\", resize_to_512 = True).cuda()\n",
    "print(img.min(), img.max())\n",
    "plt.figure()\n",
    "plt.imshow(((img[0]+1.0)/2.0*255.0).permute((1,2,0)).cpu().numpy().astype(np.uint8))\n",
    "plt.axis('off')\n",
    "\n",
    "\"\"\"# Generate segmentation masks\"\"\"\n",
    "\n",
    "config = ConfigBase()\n",
    "model = Model(config)\n",
    "init_latent = model.tensor2latent(img)\n",
    "mask_proposal = MaskProposal(config, model, init_latent)\n",
    "\n",
    "\"\"\"# Visualize image-level segmentation maps\"\"\"\n",
    "\n",
    "#Visualize pixel-level segmentation maps\n",
    "col_img = config.palette[mask_proposal.ids_img.flatten()].reshape(mask_proposal.ids_img.shape+(3,))\n",
    "im = np.array(col_img)*0.8+((img[0]+1.0)/2.0*255.0).permute((1,2,0)).cpu().numpy()*0.2\n",
    "plt.imshow(Image.fromarray(im.astype(np.uint8)))\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
