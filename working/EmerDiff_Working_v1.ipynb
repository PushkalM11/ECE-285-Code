{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from diffusers import DDIMScheduler, AutoencoderKL, UNet2DConditionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CARLA dataset\n",
    "img_path = \"/home/pushkalm11/Courses/ece285/Project/dataset/s1_2025-03-05/s1_Town01_Rep1/Town01_Scenario1_route0_03_05_14_14_18/rgb/0110.jpg\"\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image[:, 512 - 128 : 512 + 128, :]\n",
    "\n",
    "# convert to PIL image\n",
    "image = Image.fromarray(image)\n",
    "image_2 = image.resize((512, 512), resample=Image.LANCZOS)\n",
    "\n",
    "img = pil_to_tensor(image_2).cuda().unsqueeze(0).float() / 255.0 * 2.0 - 1.0\n",
    "\n",
    "# show both images side by side\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_2)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(img.min(), img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for mask proposal\n",
    "# Negative offset for mask proposal\n",
    "lambda_1 = -10 \n",
    "# Positive offset for mask proposal\n",
    "lambda_2 = 10\n",
    "# Number of segmentation masks\n",
    "num_mask = 25\n",
    "# Text prompt for mask proposal\n",
    "text_prompt = \"\"\n",
    "\n",
    "# Compression factor for VAE to produce latent space\n",
    "vae_compress_rate = 8\n",
    "# Stable Diffusion Model checkpoint\n",
    "stable_diffusion_version = \"CompVis/stable-diffusion-v1-4\"\n",
    "# CLIP Model checkpoint\n",
    "clip_version = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "# Number of timesteps for diffusion process\n",
    "inference_time_steps = 50\n",
    "\n",
    "# Timesteps for mask proposal\n",
    "# Perform multiple runs for convergence\n",
    "time_steps = [0] * 100\n",
    "# Timestep at which the feature map is injected\n",
    "inject_mask_time_stamp = [281]\n",
    "# This is the number of timesteps to use which is scaled down from the 1000 steps in stable diffusion to the inference_time_steps\n",
    "index_to_use = int(1 + (max(inject_mask_time_stamp) // (1000 / inference_time_steps)))\n",
    "\n",
    "# k-means parameters - Standard parameters\n",
    "n_init = 100\n",
    "init_algo = \"k-means++\"\n",
    "kmeans_algo = \"lloyd\"\n",
    "\n",
    "# Layers to record feature maps - This is the first cross-attention layer at the 16x16 resolution\n",
    "record_layers = [(\"cross\", \"up\", 0, 0)]\n",
    "# Layers to inject mask offsets\n",
    "inject_mask_perturbations = [(\"cross\", \"up\", 0, 2)]\n",
    "\n",
    "# EmerDiff Config Dictionary\n",
    "emerdiff_config = {\n",
    "    \"lambda_1\": lambda_1,\n",
    "    \"lambda_2\": lambda_2,\n",
    "    \"num_mask\": num_mask,\n",
    "    \"text_prompt\": text_prompt,\n",
    "    \"time_steps\": time_steps,\n",
    "    \"inject_mask_time_stamp\": inject_mask_time_stamp,\n",
    "    \"vae_compress_rate\": vae_compress_rate,\n",
    "    \"stable_diffusion_version\": stable_diffusion_version,\n",
    "    \"clip_version\": clip_version,\n",
    "    \"inference_time_steps\": inference_time_steps,\n",
    "    \"n_init\": n_init,\n",
    "    \"init_algo\": init_algo,\n",
    "    \"kmeans_algo\": kmeans_algo,\n",
    "    \"record_layers\": record_layers,\n",
    "    \"inject_mask_perturbations\": inject_mask_perturbations,\n",
    "    \"multiplication_factor\": 0.18215,\n",
    "    \"index_to_use\": index_to_use\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VAE\n",
    "VAE = AutoencoderKL.from_pretrained(\n",
    "    stable_diffusion_version, \n",
    "    subfolder = \"vae\", \n",
    "    cache_dir = \"./\").to(\"cuda\")\n",
    "\n",
    "# Load DDIM scheduler for noise addition and reverse process\n",
    "ddim = DDIMScheduler.from_pretrained(\n",
    "    stable_diffusion_version, \n",
    "    subfolder = \"scheduler\", \n",
    "    cache_dir = \"./\")\n",
    "ddim.set_timesteps(\n",
    "    inference_time_steps,\n",
    "    device = \"cuda\")\n",
    "\n",
    "# Importing CLIP parameters\n",
    "text_tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    clip_version, \n",
    "    cache_dir = \"./\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    clip_version, \n",
    "    cache_dir = \"./\").to(\"cuda\")\n",
    "\n",
    "# Importing Text-Conditioned UNet\n",
    "UNet = UNet2DConditionModel.from_pretrained(\n",
    "    stable_diffusion_version, \n",
    "    subfolder = \"unet\", \n",
    "    cache_dir = \"./\").to(\"cuda\")\n",
    "\n",
    "# k-Means processor\n",
    "kmeans_classifier = KMeans(\n",
    "    n_clusters = emerdiff_config[\"num_mask\"], \n",
    "    init = emerdiff_config[\"init_algo\"], \n",
    "    n_init = emerdiff_config[\"n_init\"], \n",
    "    random_state = 424, \n",
    "    algorithm = emerdiff_config[\"kmeans_algo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use of each flag:\n",
    "    - record_mask_embeddings: Record the mask embeddings at the correct timestep for modification\n",
    "    - record_kqv_attention: Record the original Query and Key for the attention map so that it can be used later while reconstruction\n",
    "    - use_recorded_kqv_attention: Use the original Query and Key for the attention map so that it can be used later while reconstruction\n",
    "    - perturb_feature: Perturb the feature maps\n",
    "\"\"\"\n",
    "layer_dict = {\n",
    "    # All the flags\n",
    "    \"record_mask_embeddings\": False,\n",
    "    \"record_kqv_attention\": False,\n",
    "    \"use_recorded_kqv_attention\": False,\n",
    "    \"perturb_feature\": False,\n",
    "    # Storing the attention maps\n",
    "    \"stored_attention\": defaultdict(lambda: 0),\n",
    "    \"stored_attention_count\": defaultdict(lambda: 0),\n",
    "    \"original_kqv_attention\": {},\n",
    "    # Timestep\n",
    "    \"timestep\": -1,\n",
    "    # Layers to record\n",
    "    \"record_layers\": emerdiff_config[\"record_layers\"],\n",
    "    # Layers to perturb\n",
    "    \"perturb_layers\": emerdiff_config[\"inject_mask_perturbations\"],\n",
    "    # Mask\n",
    "    \"mask\": None,\n",
    "    # Lambda\n",
    "    \"lambda_1\": emerdiff_config[\"lambda_1\"],\n",
    "    \"lambda_2\": emerdiff_config[\"lambda_2\"]\n",
    "}\n",
    "\n",
    "# Defining a class for new attention blocks to store the maps\n",
    "class NewAttentionBlock:\n",
    "    def __init__(self, layer_dict):\n",
    "        self.layer_dict = layer_dict\n",
    "    \n",
    "    # Function to store attention maps\n",
    "    def store_attention_maps(\n",
    "        self,\n",
    "        ty = \"cross\",\n",
    "        pos = \"up\",\n",
    "        res = 0,\n",
    "        idx = 0,\n",
    "        query = None,\n",
    "        key = None,\n",
    "        value = None\n",
    "    ):\n",
    "        if self.layer_dict[\"record_mask_embeddings\"]:\n",
    "            if (ty, pos, res, idx) in self.layer_dict[\"record_layers\"]:\n",
    "                fet = query[0].clone().detach().cpu() #[h*w, 768]\n",
    "                self.layer_dict[\"stored_attention\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)] += fet\n",
    "                self.layer_dict[\"stored_attention_count\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)] += 1\n",
    "            if self.layer_dict[\"record_kqv_attention\"]:\n",
    "                # Storing the original Query and Key for the attention map\n",
    "                self.layer_dict[\"original_kqv_attention\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)] = (query.detach().clone().cpu(), key.detach().clone().cpu())\n",
    "            if self.layer_dict[\"use_recorded_kqv_attention\"]:\n",
    "                stored_query, stored_key = self.layer_dict[\"original_kqv_attention\"][(self.layer_dict[\"timestep\"],ty,pos,res,idx)]\n",
    "                query[:] = stored_query[0].to(\"cuda\")\n",
    "                key[:] = stored_key[0].to(\"cuda\")\n",
    "        return query, key, value\n",
    "\n",
    "    # Function to modify the feature maps\n",
    "    def modify_feature_maps(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        ty = \"cross\",\n",
    "        pos = \"up\",\n",
    "        res = 0,\n",
    "        idx = 0,\n",
    "        to_v = None\n",
    "    ):\n",
    "        if self.layer_dict[\"perturb_feature\"] and ((ty, pos, res, idx) in self.layer_dict[\"perturb_layers\"]):\n",
    "            original_shape = hidden_states.shape\n",
    "            if len(original_shape) == 4:\n",
    "                hidden_states = hidden_states.reshape((hidden_states.shape[0], hidden_states.shape[1], -1)).permute((0, 2, 1))\n",
    "            # Reshape the mask to the same shape as the hidden states\n",
    "            mask = self.layer_dict[\"mask\"].reshape((1, -1, 1))\n",
    "            if hidden_states.shape[0] == 2:\n",
    "                lam = torch.from_numpy(np.array([self.layer_dict[\"lambda_1\"], self.layer_dict[\"lambda_2\"]])).float().reshape(2, 1, 1).to(\"cuda\")\n",
    "            else:\n",
    "                lam = torch.from_numpy(np.array([self.layer_dict[\"lambda_1\"]])).float().reshape(1, 1, 1).to(\"cuda\")\n",
    "            \n",
    "            # Perturb the feature maps - Main contribution of the EmerDiff architecture\n",
    "            hidden_states = hidden_states + lam * mask\n",
    "            if len(original_shape) == 4:\n",
    "                hidden_states = hidden_states.permute((0,2,1)).reshape(original_shape)\n",
    "        return hidden_states\n",
    "    # Function to extract out the mask features\n",
    "    def extract_mask_features(\n",
    "        self\n",
    "    ):\n",
    "        fet = []\n",
    "        for (k, v) in self.layer_dict[\"stored_attention\"].items():\n",
    "            # Here we are taking the mean of the attention maps at the correct timestep\n",
    "            # Normalization is necessary to stabilize and avoid drift\n",
    "            fet.append(v.unsqueeze(0) / self.layer_dict[\"stored_attention_count\"][k])\n",
    "        fet = torch.cat(fet, dim=0)\n",
    "        fet = fet.permute((1,0,2))\n",
    "        fet = fet.reshape((fet.shape[0], -1))\n",
    "        return fet\n",
    "\n",
    "new_attention_block = NewAttentionBlock(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been taken from an existing github repository which follows the UNet Text-conditioned architecture\n",
    "# The code has been modified to store the attention maps and modify the feature maps\n",
    "def inject_attention(unet, new_attention_block):\n",
    "    def new_forward_attention(ty = \"cross\", pos = \"up\", res=0, idx = 0):\n",
    "        def forward(attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "            batch_size, sequence_length, _ = (\n",
    "                hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "            )\n",
    "            inner_dim = hidden_states.shape[-1]\n",
    "            query = attn.to_q(hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "            if encoder_hidden_states is None:\n",
    "                encoder_hidden_states = hidden_states\n",
    "            key = attn.to_k(encoder_hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "            value = attn.to_v(encoder_hidden_states) #(batch, seq_len, num_heads*head_dim)\n",
    "            head_dim = inner_dim // attn.heads\n",
    "\n",
    "            #store qkv\n",
    "            query, key, value = new_attention_block.store_attention_maps(ty, pos, res, idx, query, key, value)\n",
    "\n",
    "            query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) #(batch, num_heads, seq_len, head_dim)\n",
    "            key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "            value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "            hidden_states = F.scaled_dot_product_attention(\n",
    "                query, key, value, attn_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim) #(batch, seq_len, num_heads*head_dim)\n",
    "            hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "            # linear proj\n",
    "            hidden_states = attn.to_out[0](hidden_states)\n",
    "\n",
    "            #perturb the output\n",
    "            hidden_states = new_attention_block.modify_feature_maps(hidden_states, ty, pos, res, idx, None)\n",
    "\n",
    "            return hidden_states\n",
    "        return forward\n",
    "    def inject_block(blocks=unet.up_blocks, pos=\"up\"):\n",
    "        #ref: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py\n",
    "        res = -1\n",
    "        if pos == \"mid\":\n",
    "            children = [blocks]\n",
    "        else:\n",
    "            children = blocks.children()\n",
    "        for net_ in children:\n",
    "            if net_.__class__.__name__ in [\"CrossAttnUpBlock2D\",\"CrossAttnDownBlock2D\",\"UNetMidBlock2DCrossAttn\"]:\n",
    "                res += 1\n",
    "                idx = -1\n",
    "                for atn in net_.attentions:\n",
    "                        if atn.__class__.__name__ == \"Transformer2DModel\":\n",
    "                            idx += 1\n",
    "                            for block in atn.transformer_blocks:\n",
    "                                if block.__class__.__name__ == \"BasicTransformerBlock\":\n",
    "                                    #self attention\n",
    "                                    if block.attn1.processor.__class__.__name__ == \"AttnProcessor2_0\":\n",
    "                                        block.attn1.processor = new_forward_attention(ty = \"self\", pos = pos, res = res, idx = idx)\n",
    "                                    #cross attention\n",
    "                                    if block.attn2.processor.__class__.__name__ == \"AttnProcessor2_0\":\n",
    "                                        block.attn2.processor = new_forward_attention(ty=\"cross\", pos = pos, res = res, idx = idx)\n",
    "        return blocks\n",
    "    unet.up_blocks = inject_block(unet.up_blocks, pos=\"up\")\n",
    "    unet.down_blocks = inject_block(unet.down_blocks, pos=\"down\")\n",
    "    unet.mid_block = inject_block(unet.mid_block, pos=\"mid\")\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet = inject_attention(UNet, new_attention_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_xts_from_x0(unet, scheduler, x0, num_inference_steps, rng):\n",
    "    alpha_bar = scheduler.alphas_cumprod\n",
    "    sqrt_one_minus_alpha_bar = (1-alpha_bar) ** 0.5\n",
    "    variance_noise_shape = (\n",
    "            num_inference_steps,\n",
    "            x0.shape[-3],\n",
    "            x0.shape[-2],\n",
    "            x0.shape[-1])\n",
    "\n",
    "    timesteps = scheduler.timesteps.to(\"cuda\")\n",
    "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
    "    xts = torch.zeros(variance_noise_shape).to(x0.device)\n",
    "    for t in reversed(timesteps):\n",
    "        idx = t_to_idx[int(t)]\n",
    "        # print(xts.shape, x0.shape)\n",
    "        xts[idx] = x0 * (alpha_bar[t] ** 0.5) + torch.randn(x0.shape, generator=rng).to(\"cuda\") * sqrt_one_minus_alpha_bar[t]\n",
    "    xts = torch.cat([xts, x0],dim = 0)\n",
    "\n",
    "    return xts\n",
    "\n",
    "def get_variance(scheduler, timestep):\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n",
    "    return variance\n",
    "\n",
    "def inversion_forward_process(unet, scheduler, x0,\n",
    "                              uncond_embedding,\n",
    "                              etas = 1.0,\n",
    "                              num_inference_steps=50,\n",
    "                              ddpm_seed = 40\n",
    "                             ):\n",
    "\n",
    "    timesteps = scheduler.timesteps.to(\"cuda\")\n",
    "    variance_noise_shape = (\n",
    "        num_inference_steps,\n",
    "        x0.shape[-3],\n",
    "        x0.shape[-2],\n",
    "        x0.shape[-1])\n",
    "    rng = torch.Generator().manual_seed(ddpm_seed)\n",
    "\n",
    "    etas = [etas]*scheduler.num_inference_steps\n",
    "    #generate noisy samples xts\n",
    "    # print(x0.shape)\n",
    "    xts = sample_xts_from_x0(unet, scheduler, x0, num_inference_steps=num_inference_steps, rng = rng)\n",
    "    alpha_bar = scheduler.alphas_cumprod\n",
    "    zs = torch.zeros(size=variance_noise_shape, device=\"cuda\") #[50, 4, 64, 64]\n",
    "\n",
    "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
    "    xt = x0\n",
    "\n",
    "    for t in timesteps:\n",
    "        idx = t_to_idx[int(t)]\n",
    "\n",
    "        xt = xts[idx][None]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = unet.forward(xt, timestep =  t, encoder_hidden_states = uncond_embedding)\n",
    "\n",
    "        noise_pred = out.sample\n",
    "\n",
    "        xtm1 =  xts[idx+1][None]\n",
    "        pred_original_sample = (xt - (1-alpha_bar[t])  ** 0.5 * noise_pred ) / alpha_bar[t] ** 0.5\n",
    "        prev_timestep = t - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "        alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "        variance = get_variance(scheduler, t)\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev - etas[idx] * variance ) ** (0.5) * noise_pred\n",
    "        mu_xt = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "        z = (xtm1 - mu_xt ) / ( etas[idx] * variance ** 0.5 )\n",
    "        zs[idx] = z\n",
    "        xtm1 = mu_xt + ( etas[idx] * variance ** 0.5 )*z\n",
    "        xts[idx+1] = xtm1\n",
    "    return xts, zs, timesteps\n",
    "\n",
    "# Simulating one step of the reverse diffusion process\n",
    "def reverse_step(scheduler, model_output, timestep, sample, eta, variance_noise):\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "    variance = get_variance(scheduler, timestep)\n",
    "    std_dev_t = eta * variance ** (0.5)\n",
    "    model_output_direction = model_output\n",
    "    pred_sample_direction = (1 - alpha_prod_t_prev - eta * variance) ** (0.5) * model_output_direction\n",
    "    prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "    if eta > 0:\n",
    "        sigma_z =  eta * variance ** (0.5) * variance_noise\n",
    "        prev_sample = prev_sample + sigma_z\n",
    "\n",
    "    return prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Process of Stable Diffusion along with UNet denoising to record attention maps\n",
    "def fwd_stable_diffusion(\n",
    "    latent, \n",
    "    time_steps, \n",
    "    new_attention_block):\n",
    "    # Random number generator which is shared across all timesteps\n",
    "    random_number_generator = torch.Generator().manual_seed(0)\n",
    "    # Adding noise to the latent vector at each timestep\n",
    "    for t in time_steps:\n",
    "        noise = torch.randn(latent.shape, generator = random_number_generator).to(\"cuda\")\n",
    "        latent_t = ddim.add_noise(latent, noise, torch.tensor([t]).int())\n",
    "        new_attention_block.layer_dict[\"timestep\"] = t\n",
    "        UNet(latent_t, t, encoder_hidden_states = new_attention_block.layer_dict[\"prompt_embedding\"])\n",
    "\n",
    "# Running one step of the reverse diffusion process\n",
    "def reverse_diffusion_one_step(\n",
    "    t, \n",
    "    latents, \n",
    "    cond1, \n",
    "    cond2 = None, \n",
    "    z_ddpm = None):\n",
    "    if cond2 != None:\n",
    "        text_embeddings  = torch.cat([cond1, cond2])\n",
    "    else:\n",
    "        text_embeddings = cond1\n",
    "    noise_pred = UNet(\n",
    "                            latents,\n",
    "                            t,\n",
    "                            encoder_hidden_states=text_embeddings\n",
    "                        ).sample\n",
    "    z_ddpm = z_ddpm.expand(latents.shape[0],-1,-1,-1) #scheduled noise\n",
    "    return reverse_step(ddim, noise_pred, t, latents, 1.0, z_ddpm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Latent Vector embedding from the image\n",
    "latent = VAE.encode(img)['latent_dist'].mean\n",
    "latent = latent * emerdiff_config[\"multiplication_factor\"]\n",
    "\n",
    "# Shape of the original image\n",
    "h, w = latent.shape[-2], latent.shape[-1]\n",
    "h, w = h * emerdiff_config[\"vae_compress_rate\"], w * emerdiff_config[\"vae_compress_rate\"]\n",
    "\n",
    "# Adding text embedding to the layer_dict\n",
    "text_prompt_tokens = text_tokenizer(\n",
    "    text_prompt, \n",
    "    padding = \"max_length\", \n",
    "    max_length = text_tokenizer.model_max_length, \n",
    "    truncation = True, \n",
    "    return_tensors = \"pt\")\n",
    "text_prompt_embeddings = text_encoder(text_prompt_tokens.input_ids.to(\"cuda\"))[0]\n",
    "print(f\"Shape of prompt embeddings in CLIP space: {text_prompt_embeddings.shape}\")\n",
    "new_attention_block.layer_dict[\"prompt_embedding\"] = text_prompt_embeddings\n",
    "# new_attention_block.layer_dict[\"prompt_embedding\"] = torch.cat([text_prompt_embeddings, text_prompt_embeddings], dim = 0)\n",
    "\n",
    "# Extracting mask for clustering\n",
    "new_attention_block.layer_dict[\"record_mask_embeddings\"] = True\n",
    "fwd_stable_diffusion(latent, emerdiff_config[\"time_steps\"], new_attention_block)\n",
    "new_attention_block.layer_dict[\"record_mask_embeddings\"] = False\n",
    "\n",
    "# Clustering the features to produce low-resolution segmentation maps\n",
    "mask_features = new_attention_block.extract_mask_features()\n",
    "kmeans_classifier.fit(mask_features)\n",
    "mask_to_id_mapping = torch.from_numpy(kmeans_classifier.labels_).to(\"cuda\")\n",
    "num_masks = mask_to_id_mapping.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the forward process and track the latents and noise at the timesteps to be used for the final image\n",
    "index_to_use = emerdiff_config[\"index_to_use\"]\n",
    "xts, zs, timesteps = inversion_forward_process(\n",
    "    UNet,\n",
    "    ddim,\n",
    "    latent,\n",
    "    new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "    etas = 1.0,\n",
    "    num_inference_steps = emerdiff_config[\"inference_time_steps\"],\n",
    "    ddpm_seed = 0\n",
    ")\n",
    "xts = xts.unsqueeze(1)\n",
    "# Excluding the first timestep\n",
    "xts = xts[-index_to_use-1:-1]\n",
    "timesteps = timesteps[-index_to_use:]\n",
    "zs = zs[-index_to_use:]\n",
    "\n",
    "# Run the reverse process to get the de-noised latents at the timesteps to be used for the final image\n",
    "with torch.no_grad():\n",
    "    new_attention_block.layer_dict[\"record_kqv_attention\"] = True\n",
    "    original_latents = xts[0]\n",
    "    for i, (xt, t) in enumerate(zip(xts, timesteps)):\n",
    "        new_attention_block.layer_dict[\"timestep\"] = t\n",
    "        original_latents = reverse_diffusion_one_step(\n",
    "            t = t,\n",
    "            latents = original_latents,\n",
    "            cond1 = new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "            z_ddpm = zs[i]\n",
    "        )\n",
    "    new_attention_block.layer_dict[\"record_kqv_attention\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the de-noised latents, we can modulate the attention maps for every mask to generate the final image\n",
    "img_with_mask_id = []\n",
    "for i in range(emerdiff_config[\"num_mask\"]):\n",
    "    print(f\"Generating the difference map for mask id: {i + 1} / {emerdiff_config['num_mask']}\")\n",
    "    with torch.no_grad():\n",
    "        # Here we generate the difference map for the mask id i and store them in a list\n",
    "        # Updating the mask\n",
    "        new_attention_block.layer_dict[\"mask\"] = (mask_to_id_mapping == i).float().to(\"cuda\")\n",
    "        \n",
    "        # Modifying the latent and generating the difference map\n",
    "        latent_to_modify = torch.cat([xts[0]]*2, dim = 0)\n",
    "        # Setting the flag to use the recorded attention maps\n",
    "        new_attention_block.layer_dict[\"use_recorded_kqv_attention\"] = True\n",
    "        for i, (xt, t) in enumerate(zip(xts, timesteps)):\n",
    "            # Setting the timestep\n",
    "            new_attention_block.layer_dict[\"timestep\"] = t\n",
    "            # Setting the flag to perturb the feature maps\n",
    "            new_attention_block.layer_dict[\"perturb_feature\"] = (t in emerdiff_config[\"inject_mask_time_stamp\"])\n",
    "            # Running the diffusion step\n",
    "            latent_to_modify = reverse_diffusion_one_step(\n",
    "                t = t,\n",
    "                latents = latent_to_modify,\n",
    "                cond1 = new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "                cond2 = new_attention_block.layer_dict[\"prompt_embedding\"],\n",
    "                z_ddpm = zs[i]\n",
    "            )\n",
    "        new_attention_block.layer_dict[\"use_recorded_kqv_attention\"] = False\n",
    "        new_attention_block.layer_dict[\"perturb_feature\"] = False\n",
    "        x_modified = VAE.decode(latent_to_modify / emerdiff_config[\"multiplication_factor\"]).sample\n",
    "        \n",
    "        # Adding blurring to the image\n",
    "        x_modified = torchvision.transforms.functional.gaussian_blur(x_modified, kernel_size = 3)\n",
    "        difference_map = torch.linalg.norm(x_modified[0 : 1] - x_modified[1 : 2], dim = 1)\n",
    "        img_with_mask_id.append(difference_map.cpu())\n",
    "    \n",
    "# Concatenating the difference maps and taking argmax to get the final image\n",
    "segmented_image = torch.argmax(torch.cat(img_with_mask_id, dim = 0), dim = 0)\n",
    "print(f\"Shape of the segmented image: {segmented_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab20', torch.max(segmented_image.flatten())+1)\n",
    "col_img = cmap(segmented_image)[:, :, :3]  # shape: (H, W, 3), values in [0,1]\n",
    "col_img = (col_img * 255).astype(np.uint8)\n",
    "plt.figure()\n",
    "plt.imshow(col_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
